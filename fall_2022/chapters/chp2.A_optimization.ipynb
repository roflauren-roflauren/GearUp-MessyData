{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Optimizing data filling via Vectorization \n",
    "\n",
    "<div>\n",
    "<img src=\"../imgs/vectorization.png\" width=\"500\"/>\n",
    "<figcaption><em>If you're too busy to read the chapter, this is basically it!</em></figcaption>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***PREFACE:***\n",
    "\n",
    "In our running examples with GDP data, we've typically dealt with datasets that are fairly small by data science standards (~1000 rows or less). \n",
    "\n",
    "Modern personal machines and shared computing resources (like Stanford's [Sherlock](https://www.sherlock.stanford.edu/)) are ridiculously powerful, and they can quickly chew through almost any code that you write - even if it has zero optimization.\n",
    "\n",
    "But what happens if you're dealing with truly massive datasets with hundreds of thousands of rows? With such datasets, code runtime can actually become a constraint on research progression and completion if processing scripts are written suboptimally.\n",
    "\n",
    "Although the defintion of optimal code and the processing of writing it varies with context, here we're going to learn a technique that can be applied in many situations: *vectorization.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***DISCUSSION 1:***\n",
    "\n",
    "A natural first question to ask is, *what is vectorizing/vectorization?* You may have heard of vectors before in a mathematics class, where vectors are a series of ordered values like this: [3, 2, 5] or [4, 7, 11, 8]. \n",
    "\n",
    "In computer science, vectorization is a technique related to the mathematical definition of a vector - it's the process of converting an algorithm from a scalar implementation (which performs operations on, at most, a pair of operands at once) to a vectorized one (which performs operations on a series of values at once). For a quick example: \n",
    "\n",
    "The processing of a scalar implementation of the instruction, \"add two to every element in this list,\" would look like: \n",
    "\n",
    "$$input: [3, 2, 5] â†’ [3 + 2 = 5, 2, 5] â†’ [5, 2 + 2 = 4, 5] â†’ [5, 4, 5 + 2 = 7] â†’ output: [5, 4, 7]$$\n",
    "\n",
    "while the processing of a vectorized implementation of the same instruction would look like: \n",
    "\n",
    "$$input: [3, 2, 5] â†’ [3 + 2 = 5, 2 + 2 = 4, 5 + 2 = 7] â†’ output: [5, 4, 7]$$\n",
    "\n",
    "(where different processors in your machine carry out the '+2' instruction on each element concurrently.)\n",
    "\n",
    "If the previous example makes sense to you, congratulations! You understand vectorization. Feel free to come up with your own quicker working definition (mine is, \"make computer do same thing to many things at same time instead of on things one-by-one.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***DISCUSSION 2:***\n",
    "\n",
    "In Python, (almost) any looped operation can be vectorized, but why do we need vectorization in the first place? \n",
    "\n",
    "<div>\n",
    "<img src=\"../imgs/pythonloop.png\" width=\"500\"/>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "While loops are a wonderful and flexible programatic idiom, they're also inherently slow due to the **dynamically typed nature of Python.** What does this mean? Let's take a look in the context of executing a program:\n",
    "\n",
    "When you execute a program in Python, Python:\n",
    "\n",
    "* First goes line-by-line through your code;\n",
    "\n",
    "* Compiles it into a machine-readable version of itself called bytecod*; and\n",
    "\n",
    "* Then this bytecode is executed to actually run the program. \n",
    "\n",
    "Suppose your code has a block where you loop over and perform some operation on the elements in a list. As Python is dynamically typed, *it does not know the type of the objects present in the list until it accesses each element.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nothing about the variable name 'a' denotes the type of the contained value. it could be: \n",
    "a = 5\n",
    "print(type(a)) # an int\n",
    "\n",
    "a = \"vectorization rocks!\" \n",
    "print(type(a)) # a string \n",
    "\n",
    "a = dict({\"vectorization\" : \"rocks!\"})\n",
    "print(type(a)) # a dictionary of strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any object in Python, the typing information is stored in the the object itself. Therefore, for each iteration in the loop, Python has to perform a series of overhead operations (like determining element type, resolving scope, checking for invalid operations, etc.) until it can carry out the actual operation you instructed it to. As you might imagine, repeatedly performing these overhead operations on massive datasets results in a significantly increased runtime. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*A quick aside:* A **statically typed** programming language like C avoids this recurring overhead cost by compelling programmers to explicitly denote the type of every object you use. But, these languages come with their own drawbacks. For example, if two external libraries provide functionality for the same concept with differently-typed implementations, library users will have to provide their own translation layer to allow the two libraries to interoperate. In Python, no/few such interoperability issues arise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The conductor says: [1 2 3 4]\n",
      "And then: [5 6 7 8]\n",
      "And the band goes: ðŸŽ¶ðŸŽ·ðŸŽ¶! ðŸŽ¶ðŸŽ¹ðŸŽ¶! ðŸŽ¶ðŸŽ»ðŸŽ¶!\n"
     ]
    }
   ],
   "source": [
    "# both pandas and numpy implement an array-type collection\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "# instantiating a numpy array and a pandas Series: \n",
    "a = np.array([1, 2, 3, 4])\n",
    "b = pd.Series([5, 6, 7, 8])\n",
    "\n",
    "# we can use both array variables at the same time, with no confusion from Python!\n",
    "print(\"The conductor says: \" + str(a))\n",
    "print(\"And then: \" + str(b.values))\n",
    "print(\"And the band goes: ðŸŽ¶ðŸŽ·ðŸŽ¶! ðŸŽ¶ðŸŽ¹ðŸŽ¶! ðŸŽ¶ðŸŽ»ðŸŽ¶!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***DISCUSSION 3:***\n",
    "\n",
    "As we continue to run through our list of interrogatives, we've learned the *what* and *why* of vectorization - now it's time to learn the *when!* \n",
    "\n",
    "If you recall, we earlier learned that almost any looped operation in Python can be vectorized. To understand the exceptions, we can first classify all looped operations into two types: \n",
    "\n",
    "1. Order-dependent operations; and\n",
    "\n",
    "2. Order-independent operations. \n",
    "\n",
    "As the names suggest, the required order (or lack thereof) of the iterations of a looped operation determines its eligibility for vectorization. Consider the following example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After imputing the 1994 data point:[(1990, 45), (1991, 46), (1992, 48), (1993, 52), (1994, 48), (1995, 55), (1996, 60), (1997, 63), (1999, 67)]\n",
      "After imputing the 1998 data point:[(1990, 45), (1991, 46), (1992, 48), (1993, 52), (1994, 48), (1995, 55), (1996, 60), (1997, 63), (1998, 56), (1999, 67)]\n",
      "Average % increase in GDP: 3.94\n"
     ]
    }
   ],
   "source": [
    "# suppose we have a Anthony-Land GDP dataset with some discontinuities: \n",
    "gdp_data = [(1990, 45), (1991, 46), (1992, 48), (1993, 52), (1995, 55), (1996, 60), (1997, 63), (1999, 67)] # data for the years 1994 and 1998 are missing! \n",
    "\n",
    "# if we want to calculate average % increase in gdp from 1990-1999, we have to fill these gaps first! \n",
    "#   fill method: gdp_X = avg. of past four years' GDP measurements. \n",
    "\n",
    "# 1994 data fill: \n",
    "gdp_data.insert(4, (1994, round((gdp_data[0][1] + gdp_data[1][1] + gdp_data[2][1] + gdp_data[3][1])/4))) \n",
    "print(\"After imputing the 1994 data point:\" + str(gdp_data))\n",
    "\n",
    "# 1998 data fill: \n",
    "gdp_data.insert(8, (1998, round((gdp_data[4][1] + gdp_data[5][1] + gdp_data[6][1] + gdp_data[7][1])/4)))\n",
    "print(\"After imputing the 1998 data point:\" + str(gdp_data))\n",
    "\n",
    "# avg % increase in gdp: \n",
    "percent_increase = []\n",
    "for i in range(len(gdp_data)): \n",
    "    if i != 0: percent_increase.append((gdp_data[i][1] - gdp_data[i-1][1])/gdp_data[i][1])\n",
    "\n",
    "print (\"Average % increase in GDP: \" + str(round(sum(percent_increase)/len(percent_increase), 4) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we computed filled the two data points in reverse-order? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After imputing the 1998 data point:[(1990, 45), (1991, 46), (1992, 48), (1993, 52), (1995, 55), (1996, 60), (1997, 63), (1998, 58), (1999, 67)]\n",
      "After imputing the 1994 data point:[(1990, 45), (1991, 46), (1992, 48), (1993, 52), (1994, 48), (1995, 55), (1996, 60), (1997, 63), (1998, 58), (1999, 67)]\n",
      "Average % increase in GDP: 4.04\n"
     ]
    }
   ],
   "source": [
    "# same data: \n",
    "gdp_data = [(1990, 45), (1991, 46), (1992, 48), (1993, 52), (1995, 55), (1996, 60), (1997, 63), (1999, 67)] # data for the years 1994 and 1998 are missing! \n",
    "\n",
    "# same fill method: gdp_X = avg. of past four years' GDP measurements. \n",
    "\n",
    "# 1998 data fill: \n",
    "gdp_data.insert(7, (1998, round((gdp_data[3][1] + gdp_data[4][1] + gdp_data[5][1] + gdp_data[6][1])/4)))\n",
    "print(\"After imputing the 1998 data point:\" + str(gdp_data))\n",
    "\n",
    "# 1994 data fill: \n",
    "gdp_data.insert(4, (1994, round((gdp_data[0][1] + gdp_data[1][1] + gdp_data[2][1] + gdp_data[3][1])/4))) \n",
    "print(\"After imputing the 1994 data point:\" + str(gdp_data))\n",
    "\n",
    "# avg % increase in gdp: \n",
    "percent_increase = []\n",
    "for i in range(len(gdp_data)): \n",
    "    if i != 0: percent_increase.append((gdp_data[i][1] - gdp_data[i-1][1])/gdp_data[i][1])\n",
    "\n",
    "print (\"Average % increase in GDP: \" + str(round(sum(percent_increase)/len(percent_increase), 4) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A-ha! As you can see, reversing the order that the datapoints were filled in does change the final resulting figure. This is because filling the 1994 data point first makes a 1994 GDP value available as a sample datapoint for imputing the 1998 GDP value, whereas reversing the order of imputation does not. \n",
    "\n",
    "As such, the operation we just performed is order-dependent (i.e., the final result is contingent upon the order in which any sub-operations are performed) and ineligible for vectorization. Reason being, *when an operation is vectorized, Python provides no constraints on the order in which the sub-operations are carried out.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"../imgs/analogy.png\" width=\"500\"/>\n",
    "<figcaption><em>Vectorized operations are to sets, as scalar looped operations are to lists!</em></figcaption>\n",
    "<div><br>\n",
    "\n",
    "Expanding upon our \"add 2 to every element in this list\" example from before, suppose our input now was much longer: $[3, 2, 5, 4, 6, 11, 14, 8, 1, 37, ... (100,000 \\text{ more numbers}) ..., 24, 2].$ Since our computer likely does not have enough processing units to add 2 to every element concurrently, it'll perform the vectorized operation in chunks. We can't guarantee that the first chunk of say, 1000 inputs, coincides with the first 1000 elements in the list. \n",
    "\n",
    "But, in cases like these, where the final result will be independent of the order of the sub-operations, this random chunking is totally fine! And for that reason, we say *order-independent* operations can be vectorized. \n",
    "\n",
    "*Note:* Determining order-independence (or lack thereof) for any particular operation is a nonstandardized task, but reference to other objects being changed by the same loop is a usually a good sign that order-dependence is present. When in doubt, work with a subset of your data and try switching up the processing order of the loop! Does the result change? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***DISCUSSION 4:***\n",
    "\n",
    "Now that we've understood the theory, it's time to tackle the practice! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **OUTLINE**\n",
    "* In Python, what tools do we use to vectorize?\n",
    "  * libraries: pandas (numpy)\n",
    "  * lineprofiler?\n",
    "  * timing programs: timeit (+ magic commands)\n",
    "   \n",
    "* How do we vectorize? *Coding demonstration* \n",
    "  * loop (Pandas, iterrows)\n",
    "  * apply (applies a function along a specified axis, rows|columns, better than iterrows, but still requires looping through rows, best used when there is no way to vectorize)\n",
    "  * vectorization in pandas\n",
    "    * side note: many pandas built-in functions are written as vectorized functions! so they're actually faster if you pass them arrays anyways\n",
    "  * vectorization with numpy arrays\n",
    "    * vec in pandas is done on pandas series (which still demands, for a given array, named indexing, data-type checking, etc.)\n",
    "    * vec in numpy ops is done \"under the hood\" as optimized, pre-compiled C code on ndarrays \n",
    "      * pd .values \n",
    "  * what if you really want to loop? \n",
    "    * why loop -> function complex, tough to vectorize; vectorize creates memory overhead, stubborn\n",
    "    * cython (converts/compiles python code as c code)\n",
    "      * %load_ext cython\n",
    "      * %%cython (-a command to show how much has been converted to C)\n",
    "      * cpdef func(): asfkjadjfkdkf\n",
    "      * run with apply\n",
    "      * making functions more cython friendly: add typing to function, replace python/numpy libraries with c math libraries\n",
    "  \n",
    "* Enjoy making your code run faster? Stay posted for the SSDS MOPS Workshop (**M**anaging & **O**ptimizing **P**erformance and **S**calability - and Memory!)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de65b0f014eaa4e91eb08470e9da2c68e352502ee239b0d825d97d29c2332a57"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
